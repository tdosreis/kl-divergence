# kl-divergence

The Kullbackâ€“Leibler divergence measures how one probability distribution P is different from a second, reference probability distribution Q. It represents the expected excess surprise from using Q as a model when the actual distribution is P.